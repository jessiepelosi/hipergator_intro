# 4. SLURM Jobs 

By the end of this tutorial you should be able to: 

* Load and use software packages (modules) on the cluster
* Assess and adjust job resources
* Write a SLURM script and submit it to the scheduler
* Check job progress and cancel jobs

## Writing and Checking SLURM Jobs

Since HiPerGator is used by thousands of people across the University, we cannot (generally) run big jobs interactively. If your command can run in less than 10 minutes, uses less than 64 Gb of RAM, and less than 16 cores, you can run it interactively on the log-in nodes (where you end up when you `ssh` into the cluster). Otherwise, you should pass your commands to the scheduler system, called SLURM, that will find the appropriate time and computer(s) to run your job. For almost everything we do, we will be going through the SLURM scheduler.  

There are few general parts to every SLURM script. You must always begin the script with this block of text:
```
#!/bin/sh
#SBATCH --job-name=your_job_name       # Job name
#SBATCH --mail-type=ALL                # Mail events (NONE, BEGIN, END, FAIL, ALL)
#SBATCH --mail-user=username@ufl.edu   # Where to send mail
#SBATCH --cpus-per-task=4              # Number of cores: Can also use -c=4
#SBATCH --mem-per-cpu=2gb              # Per processor memory
#SBATCH -t 4-00:00:00                  # Walltime
#SBATCH -o your-job-name.%j.out        # Name output file
#SBATCH --account=barbazuk             # Cluster account manager 
#SBATCH --qos=barbazuk-b               # Specify to run on burst (-b) or not 
#
```
We have to tell the scheduler how many resources and how much time the job will take. It can be hard to estimate this if you've never run a job before (or if you're running a job for a new program), but you'll get an idea about these parameters the more you use a program. In general, when I start running a program for the first time, I request 12 hours (`-t 12:00:00`), 1Gb RAM (`--mem-per-cpu=1Gb`), and 1 cpu (`--cpus-per-task=1`). When your job finishes (or fails) you'll get an email telling you how much of the resoureces you requested were used. Here's an example of what those emails tend to look like:
```
Job ID                  : 65777181
Cluster                 : hipergator
User                    : jessiepelosi
Group                   : barbazuk
State                   : COMPLETED (exit code 0)
Cores                   : 1
CPU Time Used           : 00:02:51
Wall Time Used          : 00:03:09
CPU Efficiency          : 90.48%
Memory Requested        : 1.00 GB (1.00 GB/core)
Memory Used             : 411.95 MB (411.95 MB/core estimated maximum)
Memory Efficiency       : 40.23%
```
In this case, this job completed in just over 3 minutes (Wall Time Used) and used 40.23% of the memory (RAM) requested. You can see I requested 1Gb but only 411.95Mb were needed. If I ran this job again, I would request less time (5 minutes would be enough!) and less memory (~500Mb would be good). You can use these emails to better gauge how many resources you should be requesting for your jobs. 

One thing to note is that you can run jobs on the normal queue by specifiying `--qos=barbazuk` where you'll be able to run jobs for up to thirty days, or the burst queue (`--qos=barbazuk-b`) where you have access to more resources but can only run jobs for four days. If you know your job will take 4 or fewer days, run your jobs on burst.

> **_Side Note:_** The amount of computing resources you have access to on both the regular and burst queues depends on your research group (specified by `account` and `qos`). These resources are shared by everyone in the research group, so if you are planning on running something that requires a lot of memory, it is probably a good idea to check the current resources available (see below under `squeue`) and also to check in with other members of your group so that you don't shut them out of running jobs for days.

On the next few lines you can add commands about where to work; I usually use absolute paths for this. 
```
cd /blue/barbazuk/username
```
Then you can tell HiPerGator you need to use certain programs (called "modules") that are already installed. A full list of installed programs can be found [here](https://help.rc.ufl.edu/doc/Applications). 
```
module load mafft 
```
It is usually best practice to specify which version of a program you want to load (there are often several on HiPerGator). To see more details about a program you can do the following interactively:
```
module spider mafft
```
This will bring up a short description of the program and the versions available. Now we know that we want to load mafft ver. 7.407 so we can edit our SLURM script. 
```
module load mafft/7.407
```
And finally you want to include your command! 
```
mafft --maxiterate 1000 --localpair unaligned.fasta > aligned.fasta 
```
Your full SLURM script would look like this and I like to save them with a ".slurm" file ending.  
```
#!/bin/sh
#SBATCH --job-name=your_job_name       # Job name
#SBATCH --mail-type=ALL                # Mail events (NONE, BEGIN, END, FAIL, ALL)
#SBATCH --mail-user=username@ufl.edu   # Where to send mail
#SBATCH --cpus-per-task=4              # Number of cores: Can also use -c=4
#SBATCH --mem-per-cpu=2gb              # Per processor memory
#SBATCH -t 4-00:00:00                  # Walltime
#SBATCH -o your-job-name.%j.out        # Name output file
#SBATCH --account=barbazuk             # Cluster account manager 
#SBATCH --qos=barbazuk-b               # Specify to run on burst (-b) or not 
#

cd /blue/barbazuk/username

module load mafft/7.407

mafft --maxiterate 1000 --localpair unaligned.fasta > aligned.fasta 
```

To submit your SLURM job: 
```
sbatch myslurm.slurm
```

To check on your job or other jobs: 
```
squeue -A barbazuk       # will print all the jobs on the Barbazuk account 
squeue --qos barbazuk-b  # will print all the jobs ont he Barbazuk burst queue 
squeue -U username       # will print all of your jobs 
``` 

If you need to cancel a job for any reason (you can get the jobID from the above commands): 
```
scancel jobID
```

# Exercises 
Go to the [list of HiperGator Applications](https://help.rc.ufl.edu/doc/Applications/) and pick one that you think sounds interesting. Write a SLURM job with the following parameters:
* Has the job name "test"
* Sends an alert to your UF email when it starts and stops
* Uses 1 `cpus-per-task`
* Uses 5 Mb per cpu
* Uses 5 minutes of run time
* Saves a job output report called "test"
* Is run on the burst queue of your main research group's account

In the body of the script, load the latest version of whichever program you picked above. Then print the text "This is my first SLURM job!". (Printing to the terminal can be done in shell with the command `echo`.)

Submit your job. If you don't feel confident that all of it is correct, feel free to send it to your mentor first! When your job is done, take a look at the email report that you got and the output report in your HiperGator folder.
